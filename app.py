import streamlit as st
import os
import tempfile
import datetime
from openai import OpenAI
import google.generativeai as genai

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="AIè­°äº‹éŒ²Pro", page_icon="ğŸ“")

st.title("ğŸ“ AIçµ±åˆãƒ¬ãƒãƒ¼ãƒˆä½œæˆãƒ„ãƒ¼ãƒ«")
st.caption("OpenAI Whisperã‚’ä½¿ç”¨ã—ã¦æ–‡å­—èµ·ã“ã—ã‚’è¡Œã„ã¾ã™ï¼ˆ25MBåˆ¶é™ã‚ã‚Šï¼‰")

# --- ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ– ---
if "report_text" not in st.session_state:
    st.session_state.report_text = None
if "full_transcript" not in st.session_state:
    st.session_state.full_transcript = None

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šè¨­å®š ---
with st.sidebar:
    st.header("ğŸ”‘ è¨­å®š")
    openai_key = st.text_input("OpenAI API Key", type="password")
    gemini_key = st.text_input("Gemini API Key", type="password")
    
    report_type = st.radio("ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆã®ç¨®é¡", ["ä¼šè­°ãƒ»æ‰“ã¡åˆã‚ã›", "è¬›æ¼”ä¼šãƒ»ã‚»ãƒŸãƒŠãƒ¼", "ç›¸è«‡ä¼šãƒ»ãƒ’ã‚¢ãƒªãƒ³ã‚°"])
    selected_model = st.selectbox("ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«", ["gemini-1.5-flash", "gemini-1.5-pro"])

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
uploaded_files = st.file_uploader("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["mp3", "m4a", "wav"], accept_multiple_files=True)

if uploaded_files and openai_key and gemini_key:
    uploaded_files.sort(key=lambda x: x.name)
    
    if st.button("ğŸš€ ãƒ¬ãƒãƒ¼ãƒˆä½œæˆã‚’é–‹å§‹"):
        progress_bar = st.progress(0)
        client = OpenAI(api_key=openai_key)
        genai.configure(api_key=gemini_key)
        model = genai.GenerativeModel(selected_model)
        
        full_transcript = ""
        
        try:
            for i, uploaded_file in enumerate(uploaded_files):
                st.info(f"æ–‡å­—èµ·ã“ã—ä¸­: {uploaded_file.name}")
                
                # ä¸€æ™‚ä¿å­˜ã—ã¦Whisperã«æŠ•ã’ã‚‹
                with tempfile.NamedTemporaryFile(delete=False, suffix=f".{uploaded_file.name.split('.')[-1]}") as tmp:
                    tmp.write(uploaded_file.getvalue())
                    tmp_path = tmp.name
                
                with open(tmp_path, "rb") as audio_file:
                    transcript = client.audio.transcriptions.create(
                        model="whisper-1", 
                        file=audio_file,
                        response_format="text"
                    )
                
                os.remove(tmp_path)
                full_transcript += f"\n\n--- {uploaded_file.name} ---\n{transcript}"
                progress_bar.progress((i + 1) / (len(uploaded_files) + 1))

            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            st.info("ãƒ¬ãƒãƒ¼ãƒˆåŸ·ç­†ä¸­...")
            today = datetime.date.today().strftime('%Y-%m-%d')
            response = model.generate_content(f"æ—¥ä»˜:{today}\nå†…å®¹:\n{full_transcript}")
            
            st.session_state.report_text = response.text
            st.session_state.full_transcript = full_transcript
            progress_bar.progress(100)
            
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

# --- çµæœè¡¨ç¤º ---
if st.session_state.report_text:
    st.markdown(st.session_state.report_text)
    with st.expander("æ–‡å­—èµ·ã“ã—åŸæ–‡ã‚’ç¢ºèª"):
        st.text_area("åŸæ–‡", st.session_state.full_transcript, height=200)
